{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revised to use open_clip instead of clip\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "#from clip import load\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def benchmark_model(model_name, benchmark_dir,pretrained='openai',checkpoint_dir=None, device = \"cpu\"):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained,force_quick_gelu=True)\n",
    "    tokenizer=open_clip.get_tokenizer(model_name)\n",
    "    if checkpoint_dir is not None:\n",
    "        checkpoint =torch.load(checkpoint_dir, map_location='cpu')\n",
    "        # loading a bare (model only) checkpoint for fine-tune or evaluation\n",
    "        sd = checkpoint[\"state_dict\"]\n",
    "        if next(iter(sd.items()))[0].startswith('module'):\n",
    "            sd = {k[len('module.'):]: v for k, v in sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "\n",
    "    model.to(device)\n",
    "    image_dir = os.path.join(benchmark_dir, 'MLLM_VLM Images')\n",
    "    csv_file = os.path.join(benchmark_dir, 'Questions.csv')\n",
    "    \n",
    "\n",
    "    csv_outfile = open('output.csv', 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_outfile)\n",
    "    csv_writer.writerow(['qid1', 'qid2', 'pred1', 'pred2', 'gt1', 'gt2', 'q1score', 'q2score','img_sim','text_sim'])  # header\n",
    "\n",
    "    categories = [\n",
    "        'Orientation and Direction', 'Presence of Specific Features', \n",
    "        'State and Condition', 'Quantity and Count', \n",
    "        'Positional and Relational Context', 'Color and Appearance',\n",
    "        'Structural Characteristics', 'Texts',\n",
    "        'Viewpoint and Perspective'\n",
    "    ]\n",
    "\n",
    "    pair_accuracies = {category: 0 for category in categories}\n",
    "    text_similarities={category: 0 for category in categories}\n",
    "    img_similarities={category: 0 for category in categories}\n",
    "    num_pairs = 0\n",
    "    \n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for i, row in tqdm(enumerate(reader)):\n",
    "            qid1, qtype1, statement1 = row\n",
    "        \n",
    "            # Get next row for the pair\n",
    "            row = next(reader, None)\n",
    "            if not row:\n",
    "                break\n",
    "            qid2, qtype2, statement2 = row\n",
    "            \n",
    "            qid1, qid2 = int(qid1), int(qid2)\n",
    "            \n",
    "            img1 = Image.open(os.path.join(image_dir, qtype1, f'{qid1}.jpg'))\n",
    "            img2 = Image.open(os.path.join(image_dir, qtype1, f'{qid2}.jpg'))\n",
    "\n",
    "            text1 = 'a photo of ' + statement1\n",
    "            text2 = 'a photo of ' + statement2\n",
    "\n",
    "            \n",
    "            #texts = tokenizer([text1,text2]).to(device)\n",
    "            text1 = tokenizer([text1]).to(device)\n",
    "            text2 = tokenizer([text2]).to(device)\n",
    "            \n",
    "            img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "            img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "            imgs = torch.cat((img1, img2), dim=0)\n",
    "      \n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(imgs)\n",
    "               \n",
    "                text_features1 = model.encode_text(text1)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features1 /= text_features1.norm(dim=-1, keepdim=True)\n",
    "                # logits_per_image1, logits_per_text1 = model(imgs, text1)\n",
    "                # logits_per_image2, logits_per_text2 = model(imgs, text2)\n",
    "                \n",
    "                # probs1 = logits_per_text1.softmax(dim=-1).cpu().numpy()\n",
    "                # probs2 = logits_per_text2.softmax(dim=-1).cpu().numpy()\n",
    "                sim_img=image_features@image_features.T\n",
    "                sim_img=sim_img[0][1]\n",
    "                \n",
    "                probs1 = (100.0 * image_features @ text_features1.T).softmax(dim=0)\n",
    "              \n",
    "\n",
    "                image_features = model.encode_image(imgs)\n",
    "                text_features2 = model.encode_text(text2)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features2 /= text_features2.norm(dim=-1, keepdim=True)\n",
    "                # logits_per_image1, logits_per_text1 = model(imgs, text1)\n",
    "                # logits_per_image2, logits_per_text2 = model(imgs, text2)\n",
    "                \n",
    "                # probs1 = logits_per_text1.softmax(dim=-1).cpu().numpy()\n",
    "                # probs2 = logits_per_text2.softmax(dim=-1).cpu().numpy()\n",
    "                probs2 = (100.0 * image_features @ text_features2.T).softmax(dim=0)\n",
    "                sim_text=text_features1@text_features2.T\n",
    "                sim_text=sim_text[0][0]\n",
    "                \n",
    "  \n",
    "            img1_score1 = probs1[0][0]\n",
    "            img1_score2 = probs2[0][0]\n",
    "            \n",
    "            pred1 = \"img1\" if img1_score1 > 0.5 else \"img2\"\n",
    "            pred2 = \"img1\" if img1_score2 > 0.5 else \"img2\"\n",
    "\n",
    "            gt1 = \"img1\" if qid1 % 2 == 1 else \"img2\"\n",
    "            gt2 = \"img1\" if qid2 % 2 == 1 else \"img2\"\n",
    "\n",
    "            \n",
    "            csv_writer.writerow([qid1, qid2, pred1, pred2, gt1, gt2, img1_score1, img1_score2,sim_img,sim_text])\n",
    "                \n",
    "            current_category = categories[num_pairs // 15]\n",
    "            if pred1 == gt1 and pred2 == gt2:\n",
    "                pair_accuracies[current_category] += 1\n",
    "            text_similarities[current_category]+=sim_text\n",
    "            img_similarities[current_category]+=sim_img\n",
    "            num_pairs += 1\n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "        csv_outfile.close()\n",
    "\n",
    "    # Calculate percentage accuracies\n",
    "    for category in pair_accuracies:\n",
    "        pair_accuracies[category] = (pair_accuracies[category] / (num_pairs // len(categories))) * 100\n",
    "        text_similarities[category]=text_similarities[category]/(num_pairs // len(categories))\n",
    "        img_similarities[category]=img_similarities[category]/(num_pairs // len(categories))\n",
    "\n",
    "    return pair_accuracies,text_similarities,img_similarities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='ViT-L-14' \n",
    "MMVP_dir='./MMVP_VLM'\n",
    "checkpoint_dir='./weights/ViT_stage_1_epoch_20.pt'\n",
    "result,_,_=benchmark_model(model,MMVP_dir,pretrained='openai',device=torch.device('cuda:0'),checkpoint_dir=checkpoint_dir)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revised to use longclip instead of clip\n",
    "from model import longclip\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def benchmark_model(model_name, benchmark_dir,pretrained='openai',checkpoint_dir=None, device = \"cpu\"):\n",
    "    model, preprocess = longclip.load(checkpoint_dir,device=device)\n",
    "    #tokenizer=open_clip.get_tokenizer(model_name)\n",
    "    \n",
    "\n",
    "    #model.to(device)\n",
    "    image_dir = os.path.join(benchmark_dir, 'MLLM_VLM Images')\n",
    "    csv_file = os.path.join(benchmark_dir, 'Questions.csv')\n",
    "    \n",
    "\n",
    "    csv_outfile = open('output.csv', 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_outfile)\n",
    "    csv_writer.writerow(['qid1', 'qid2', 'pred1', 'pred2', 'gt1', 'gt2', 'q1score', 'q2score','img_sim','text_sim'])  # header\n",
    "\n",
    "    categories = [\n",
    "        'Orientation and Direction', 'Presence of Specific Features', \n",
    "        'State and Condition', 'Quantity and Count', \n",
    "        'Positional and Relational Context', 'Color and Appearance',\n",
    "        'Structural Characteristics', 'Texts',\n",
    "        'Viewpoint and Perspective'\n",
    "    ]\n",
    "\n",
    "    pair_accuracies = {category: 0 for category in categories}\n",
    "    text_similarities={category: 0 for category in categories}\n",
    "    img_similarities={category: 0 for category in categories}\n",
    "    num_pairs = 0\n",
    "    \n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for i, row in tqdm(enumerate(reader)):\n",
    "            qid1, qtype1, statement1 = row\n",
    "        \n",
    "            # Get next row for the pair\n",
    "            row = next(reader, None)\n",
    "            if not row:\n",
    "                break\n",
    "            qid2, qtype2, statement2 = row\n",
    "            \n",
    "            qid1, qid2 = int(qid1), int(qid2)\n",
    "            \n",
    "            img1 = Image.open(os.path.join(image_dir, qtype1, f'{qid1}.jpg'))\n",
    "            img2 = Image.open(os.path.join(image_dir, qtype1, f'{qid2}.jpg'))\n",
    "\n",
    "            text1 = 'a photo of ' + statement1\n",
    "            text2 = 'a photo of ' + statement2\n",
    "\n",
    "            \n",
    "            #texts = tokenizer([text1,text2]).to(device)\n",
    "            text1 = longclip.tokenize([text1]).to(device)\n",
    "            text2 = longclip.tokenize([text2]).to(device)\n",
    "            \n",
    "            img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "            img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "            imgs = torch.cat((img1, img2), dim=0)\n",
    "      \n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(imgs)\n",
    "               \n",
    "                text_features1 = model.encode_text(text1)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features1 /= text_features1.norm(dim=-1, keepdim=True)\n",
    "                # logits_per_image1, logits_per_text1 = model(imgs, text1)\n",
    "                # logits_per_image2, logits_per_text2 = model(imgs, text2)\n",
    "                \n",
    "                # probs1 = logits_per_text1.softmax(dim=-1).cpu().numpy()\n",
    "                # probs2 = logits_per_text2.softmax(dim=-1).cpu().numpy()\n",
    "                sim_img=image_features@image_features.T\n",
    "                sim_img=sim_img[0][1]\n",
    "                \n",
    "                probs1 = (100.0 * image_features @ text_features1.T).softmax(dim=0)\n",
    "              \n",
    "\n",
    "                image_features = model.encode_image(imgs)\n",
    "                text_features2 = model.encode_text(text2)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features2 /= text_features2.norm(dim=-1, keepdim=True)\n",
    "                # logits_per_image1, logits_per_text1 = model(imgs, text1)\n",
    "                # logits_per_image2, logits_per_text2 = model(imgs, text2)\n",
    "                \n",
    "                # probs1 = logits_per_text1.softmax(dim=-1).cpu().numpy()\n",
    "                # probs2 = logits_per_text2.softmax(dim=-1).cpu().numpy()\n",
    "                probs2 = (100.0 * image_features @ text_features2.T).softmax(dim=0)\n",
    "                sim_text=text_features1@text_features2.T\n",
    "                sim_text=sim_text[0][0]\n",
    "                \n",
    "  \n",
    "            img1_score1 = probs1[0][0]\n",
    "            img1_score2 = probs2[0][0]\n",
    "            \n",
    "            pred1 = \"img1\" if img1_score1 > 0.5 else \"img2\"\n",
    "            pred2 = \"img1\" if img1_score2 > 0.5 else \"img2\"\n",
    "\n",
    "            gt1 = \"img1\" if qid1 % 2 == 1 else \"img2\"\n",
    "            gt2 = \"img1\" if qid2 % 2 == 1 else \"img2\"\n",
    "\n",
    "            \n",
    "            csv_writer.writerow([qid1, qid2, pred1, pred2, gt1, gt2, img1_score1, img1_score2,sim_img,sim_text])\n",
    "                \n",
    "            current_category = categories[num_pairs // 15]\n",
    "            if pred1 == gt1 and pred2 == gt2:\n",
    "                pair_accuracies[current_category] += 1\n",
    "            text_similarities[current_category]+=sim_text\n",
    "            img_similarities[current_category]+=sim_img\n",
    "            num_pairs += 1\n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "        csv_outfile.close()\n",
    "\n",
    "    # Calculate percentage accuracies\n",
    "    for category in pair_accuracies:\n",
    "        pair_accuracies[category] = (pair_accuracies[category] / (num_pairs // len(categories))) * 100\n",
    "        text_similarities[category]=text_similarities[category]/(num_pairs // len(categories))\n",
    "        img_similarities[category]=img_similarities[category]/(num_pairs // len(categories))\n",
    "\n",
    "    return pair_accuracies,text_similarities,img_similarities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate based on the concatenation of the representations from a list of clip models\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "#from clip import load\n",
    "\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def benchmark_model(benchmark_dir,model_name,pretrained='openai',checkpoint_dirs=[], device = \"cpu\"):\n",
    "    models=[]\n",
    "    for checkpoint_dir in checkpoint_dirs:\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained,force_quick_gelu=True)\n",
    "        tokenizer=open_clip.get_tokenizer(model_name)\n",
    "        if checkpoint_dir is not None:\n",
    "            checkpoint =torch.load(checkpoint_dir, map_location='cpu')\n",
    "            # loading a bare (model only) checkpoint for fine-tune or evaluation\n",
    "            sd = checkpoint[\"state_dict\"]\n",
    "            if next(iter(sd.items()))[0].startswith('module'):\n",
    "                sd = {k[len('module.'):]: v for k, v in sd.items()}\n",
    "            model.load_state_dict(sd)\n",
    "        model.to(device)\n",
    "        models.append(model)\n",
    "\n",
    "    image_dir = os.path.join(benchmark_dir, 'MLLM_VLM Images')\n",
    "    csv_file = os.path.join(benchmark_dir, 'Questions.csv')\n",
    "    \n",
    "\n",
    "    csv_outfile = open('output.csv', 'w', newline='')\n",
    "    csv_writer = csv.writer(csv_outfile)\n",
    "    csv_writer.writerow(['qid1', 'qid2', 'pred1', 'pred2', 'gt1', 'gt2', 'q1score', 'q2score','img_sim','text_sim'])  # header\n",
    "\n",
    "    categories = [\n",
    "        'Orientation and Direction', 'Presence of Specific Features', \n",
    "        'State and Condition', 'Quantity and Count', \n",
    "        'Positional and Relational Context', 'Color and Appearance',\n",
    "        'Structural Characteristics', 'Texts',\n",
    "        'Viewpoint and Perspective'\n",
    "    ]\n",
    "\n",
    "    pair_accuracies = {category: 0 for category in categories}\n",
    "    text_similarities={category: 0 for category in categories}\n",
    "    img_similarities={category: 0 for category in categories}\n",
    "    num_pairs = 0\n",
    "    \n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # skip header\n",
    "        for i, row in tqdm(enumerate(reader)):\n",
    "            qid1, qtype1, statement1 = row\n",
    "        \n",
    "            # Get next row for the pair\n",
    "            row = next(reader, None)\n",
    "            if not row:\n",
    "                break\n",
    "            qid2, qtype2, statement2 = row\n",
    "            \n",
    "            qid1, qid2 = int(qid1), int(qid2)\n",
    "            \n",
    "            img1 = Image.open(os.path.join(image_dir, qtype1, f'{qid1}.jpg'))\n",
    "            img2 = Image.open(os.path.join(image_dir, qtype1, f'{qid2}.jpg'))\n",
    "\n",
    "            text1 = 'a photo of ' + statement1\n",
    "            text2 = 'a photo of ' + statement2\n",
    "\n",
    "            \n",
    "            #texts = tokenizer([text1,text2]).to(device)\n",
    "            text1 = tokenizer([text1]).to(device)\n",
    "            text2 = tokenizer([text2]).to(device)\n",
    "            \n",
    "            img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "            img2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "            imgs = torch.cat((img1, img2), dim=0)\n",
    "      \n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features = [models[k].encode_image(imgs) for k in range(len(models))]\n",
    "                image_features=torch.cat(image_features,dim=1)\n",
    "               \n",
    "                text_features1 = [models[k].encode_text(text1) for k in range(len(models))]\n",
    "                text_features1=torch.cat(text_features1,dim=1)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features1 /= text_features1.norm(dim=-1, keepdim=True)\n",
    "                # logits_per_image1, logits_per_text1 = model(imgs, text1)\n",
    "                # logits_per_image2, logits_per_text2 = model(imgs, text2)\n",
    "                \n",
    "                # probs1 = logits_per_text1.softmax(dim=-1).cpu().numpy()\n",
    "                # probs2 = logits_per_text2.softmax(dim=-1).cpu().numpy()\n",
    "                sim_img=image_features@image_features.T\n",
    "                sim_img=sim_img[0][1]\n",
    "                \n",
    "                probs1 = (100.0 * image_features @ text_features1.T).softmax(dim=0)\n",
    "              \n",
    "\n",
    "                #image_features = model.encode_image(imgs)\n",
    "                text_features2 = [models[k].encode_text(text2) for k in range(len(models))]\n",
    "                text_features2=torch.cat(text_features2,dim=1)\n",
    "                #image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "                text_features2 /= text_features2.norm(dim=-1, keepdim=True)\n",
    "                # logits_per_image1, logits_per_text1 = model(imgs, text1)\n",
    "                # logits_per_image2, logits_per_text2 = model(imgs, text2)\n",
    "                \n",
    "                # probs1 = logits_per_text1.softmax(dim=-1).cpu().numpy()\n",
    "                # probs2 = logits_per_text2.softmax(dim=-1).cpu().numpy()\n",
    "                probs2 = (100.0 * image_features @ text_features2.T).softmax(dim=0)\n",
    "                sim_text=text_features1@text_features2.T\n",
    "                sim_text=sim_text[0][0]\n",
    "                \n",
    "  \n",
    "            img1_score1 = probs1[0][0]\n",
    "            img1_score2 = probs2[0][0]\n",
    "            \n",
    "            pred1 = \"img1\" if img1_score1 > 0.5 else \"img2\"\n",
    "            pred2 = \"img1\" if img1_score2 > 0.5 else \"img2\"\n",
    "\n",
    "            gt1 = \"img1\" if qid1 % 2 == 1 else \"img2\"\n",
    "            gt2 = \"img1\" if qid2 % 2 == 1 else \"img2\"\n",
    "\n",
    "            \n",
    "            csv_writer.writerow([qid1, qid2, pred1, pred2, gt1, gt2, img1_score1, img1_score2,sim_img,sim_text])\n",
    "                \n",
    "            current_category = categories[num_pairs // 15]\n",
    "            if pred1 == gt1 and pred2 == gt2:\n",
    "                pair_accuracies[current_category] += 1\n",
    "            text_similarities[current_category]+=sim_text\n",
    "            img_similarities[current_category]+=sim_img\n",
    "            num_pairs += 1\n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "        csv_outfile.close()\n",
    "\n",
    "    # Calculate percentage accuracies\n",
    "    for category in pair_accuracies:\n",
    "        pair_accuracies[category] = (pair_accuracies[category] / (num_pairs // len(categories))) * 100\n",
    "        text_similarities[category]=text_similarities[category]/(num_pairs // len(categories))\n",
    "        img_similarities[category]=img_similarities[category]/(num_pairs // len(categories))\n",
    "\n",
    "    return pair_accuracies,text_similarities,img_similarities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model='ViT-L-14' \n",
    "MMVP_dir='./MMVP_VLM'\n",
    "checkpoint_dirs=[None,\n",
    "    './weights/ViT_stage_1_epoch_20.pt',\n",
    "    './weights/ViT_stage_2_epoch_20.pt',\n",
    "    './weights/ViT_stage_3_epoch_20.pt']\n",
    "\n",
    "result,_,_=benchmark_model(MMVP_dir,model_name=model,device=torch.device('cuda:0'),checkpoint_dirs=checkpoint_dirs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
